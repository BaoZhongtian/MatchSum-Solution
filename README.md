# MatchSum-Solution
 This is the study case of MatchSum Model. And hope it could help more researchers.

This code is based on the MatchSum Code (https://github.com/maszhongming/MatchSum)

And I added BertSum model to select potential sentences before MatchSum Search.

本项目为对MatchSum Model的学习案例，并希望该案例能帮助其他研究者。

本项目的代码基于在GitHub上开源的MatchSum代码(https://github.com/maszhongming/MatchSum)

同时，本项目增加了BertSum模型来选择关键句子方便后续进行MatchSum搜索。

# Neural Network Parameter Download
The Trained BertSum Parameter could be download from https://drive.google.com/file/d/1Pz7r7ynF2cUuMQsZQ2jOZ7DONzyqwLh4/view?usp=sharing

The Trained MatchSum Parameter could be download from https://github.com/maszhongming/MatchSum

本项目使用的训练完毕BertSum模型参数可下载于https://drive.google.com/file/d/1Pz7r7ynF2cUuMQsZQ2jOZ7DONzyqwLh4/view?usp=sharing

本项目使用的训练完毕MatchSum模型参数来源于https://github.com/maszhongming/MatchSum

# Description
The new two generated files are Train/BertSumExperiment.py and End2End_Server.py

Train/BertSumExperiment.py file is to use Bert-Base-Uncased model to generate tag information.
This tag information is provided by MatchSum(https://github.com/maszhongming/MatchSum)

End2End_Server.py file is to treat the input Extractive Summarization Request in 'Request Fold'.
The treat method is based on the MatchSum Paper:

First using BertSum model to extract Top-N probability sentences.
Then using MatchSum model to search the Extractive Summarization.

本项目主要新增的文件为Train/BertSumExperiment.py以及End2End_Server.py两个可运行程序。

Train/BertSumExperiment.py为根据MatchSum给出的ID标注信息，使用Bert-Base-Uncased结构来进行微调。

End2End_Server.py为对Request文件夹下面的文件进行处理，处理方式参照MatchSum论文的方式：

先使用BertSum提取Top-N概率的句子，再以此为范围进行MatchSum摘要的搜索。

# Input File Description
The Input File Demo could be seen in demo/input_document.txt

Remind: Each sentence should be separated by \n.

输入文件案例为demo/input_document.txt

注意：一行只能包含一句，每一句之间以换行符（\n）进行分隔。

# Output File Description
The Output File Demo could be seen in demo/input_document_predict.txt

There are three dictionary keys in the Output File:

BertSumSelect: which sentences are selected in BertSum Model.

Scores: different combinations' score calculated by MatchSum model.

Top-N: Top-N predicts generated by MatchSum model.

输出文件案例为demo/input_document_predict.txt

在该文件之中，包含如下信息：

BertSumSelect: BertSum模型处理之后，所挑选的Sentence ID。

Scores：MatchSum模型对不同组合评估出的分数。

Top-N：MatchSum给出的Top-N个抽取式摘要结果。

# Performance

| Model | Rouge-1 | Rouge-2 | Rouge-L |
| :------ | :------: | :------: | :------: |
| BertSum+MatchSum (BERT-base) | 42.1938 | 19.6615 | 27.5057 |


The reason of BertSum+MatchSum(BERT-base) performance lower than (https://github.com/maszhongming/MatchSum) is BertSum model changes.

In the Evaluate Part, I use BertSum model trained by only CNN/DM dataset and the program is based on Transformers Repository rather than existed model.

对于BertSum+MatchSum(BERT-base)与(https://github.com/maszhongming/MatchSum)
相比性能更低的原因，经过分析为BertSum部分进行了调整。

在性能指标评估部分，本项目只使用CNN/DM的数据对BertSum进行了训练，并直接使用Transformers库进行重新编写，而非使用现有公开的模型。